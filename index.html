<!doctype html>
<html>
<head>
  <title> A.I </title>
  <meta charset="utf-8">
  <style>
    body {
      margin: 0px;
    }
    a {
      color: black;
      text-decoration: none;
    }
    h1 {
      font-size: 40px;
      text-align: center;
      border-bottom: solid gray 1px;
      margin: 0;
      padding: 10px;
    }
  </style>
</head>
<body>
  <h1>Artificial Intelligence (gipark2001@naver.com)</h1>
  <a href="ai_machine_deep.html" style="color:blue;"><h2>  1. Artificial Intelligence vs Machine Learning vs Deep Learning</h2></a>
  <h2>  2.Type of learning algorithms </h2>
  <ul>
    <li><a href="super_learning.html" style="color:blue;"><h3>Supervised learning</h3></a></li>
    <li><a href="unsuper_learning.html" style="color:blue;"><h3>Unsupervised learning</h3></a></li>
    <li><a href="reinfor_learning.html" style="color:blue;"><h3>Reinforcement learning</h3></a></li>
  </ul>
  <img src="super_unsuper_rein.jpg" width="500">
  <h2><a href="deep_learning.html"></a>3. Deep Learning </h2>
  <ol>
    <li><a href="machine_neural.html" style="color:blue">Difference between Machin Learning and Neural Network</a></li>
    <li>History of Neural Nets</li>
    <p>
      It all started when Warren McCulloch and Walter Pitts created the first model of an NN in 1943. Their model was purely based on mathematics and algorithms and couldn’t be tested due to the lack of computational resources.
      Later on, in 1958, Frank Rosenblatt created the first ever model that could do pattern recognition. This would change it all. The Perceptron. However, he only gave the notation and the model. The actual model still could not be tested. There were relatively minor researches done before this.
      The first NNs that could be tested and had many layers were published by Alexey Ivakhnenko and Lapa in 1965.
      After these, the research on NNs stagnated due to high feasibility of Machine Learning models. This was done by Marvin Minsky and Seymour Papert in 1969.
      This stagnation however, was relatively short-termed as 6 years later in 1975 Paul Werbos came up with Back-propagation, which solved the XOR problem and in general made NN learning more efficient.
      Max-pooling was later introduced in 1992 which helped with 3D object recognition as it helped with least shift invariance and tolerance to deformation.
      Between 2009 and 2012, Recurrent NNs and Deep Feed Forward NNs created by Jürgen Schmidhuber’s research group went on to win 8 international competitions in pattern recognition and machine learning.
      In 2011, Deep NNs started incorporating convolutional layers with max-pooling layers whose output was then passed to several fully connected layers which were followed by an output layer. These are called Convolutional Neural Networks.they aggregate and recombine features from the previous layer.
    </p>
  </ol>
</body>
</html>
