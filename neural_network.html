<!DOCTYPE html>
<html>
  <head>
    <title> Neural Network</title>
    <meta name="viewport" content="width=device-width">
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="css/neural_network.css">
  </head>
  <body>
    <ol>
      <article class="perceptron">
        <li>Perceptrons</li>
        <p>
          Deep learning is the name we use for “stacked neural networks”; that is, networks composed of several layers.
          The layers are made of nodes. A node is just a place where computation happens, loosely patterned on a neuron in the human brain, which fires when it encounters sufficient stimuli. A node combines input from the data with a set of coefficients, or weights, that either amplify or dampen that input, thereby assigning significance to inputs with regard to the task the algorithm is trying to learn; e.g. which input is most helpful is classifying data without error? These input-weight products are summed and then the sum is passed through a node’s so-called activation function, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome, say, an act of classification. If the signals passes through, the neuron has been “activated.”
          Here’s a diagram of what one node might look like.
        </p>
        <img src="img/perceptron.jpg">
        <p>
          A node layer is a row of those neuron-like switches that turn on or off as the input is fed through the net. Each layer’s output is simultaneously the subsequent layer’s input, starting from an initial input layer receiving your data.
        </p>
        <img src="img/connected_neuron.jpg">
      </article>
      <article class="concept">
        <li>Key Concepts of Deep Neural Networks</li>
        <p>
          Deep-learning networks are distinguished from the more commonplace single-hidden-layer neural networks by their depth; that is, the number of node layers through which data must pass in a multistep process of pattern recognition.
          Earlier versions of neural networks such as the first perceptrons were shallow, composed of one input and one output layer, and at most one hidden layer in between. More than three layers (including input and output) qualifies as “deep” learning. So deep is not just a buzzword to make algorithms seem like they read Sartre and listen to bands you haven’t heard of yet. It is a strictly defined term that means more than one hidden layer.
          In deep-learning networks, each layer of nodes trains on a distinct set of features based on the previous layer’s output. The further you advance into the neural net, the more complex the features your nodes can recognize, since they aggregate and recombine features from the previous layer.
        </p>
        <img src="img/deeper_layer_representation.jpg">
      </article>
    </ol>
  </body>
</html>
