<!DOCTYPE html>
<html>
  <head>
    <title> Neural Network</title>
    <meta name="viewport" content="width=device-width">
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="css/neural_network.css">
  </head>
  <body>
    <ol>
      <div class="perceptron">
        <h2><li>Perceptrons</li></h2>
        <p>
          The architecture and behavior of a perceptron is very similar to biological neurons, and is often considered as the most basic form of neural network. Other kinds of neural networks were developed after the perceptron, and their diversity and applications continue to grow. It is easier to explain the constitutes of a neural network using the example of a single layer perceptron.
        </p>
        <img src="img/perceptron.jpg">
        <p>
          A single layer perceptron works as a linear binary classifier. Consider a feature vector [x1, x2, x3] that is used to predict the probability (p) of occurrence of a certain event.
        </p>
      </div>
      <div class="weight_factor">
        <h2><li>Weighing factors</li></h2>
        <p>
          Each input in the feature vector is assigned its own relative weight (w), which decides the impact that the particular input needs in the summation function. In relatively easier terms, some inputs are made more important than others by giving them more weight so that they have a greater effect in the summation function (y). A bias (wo) is also added to the summation.
        </p>
        <img src="img/weight_factor.jpg">
      </div>
      <div class="activation">
        <h2><li>Activation function</li> </h2>
        <p>
          Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.
          Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.
        </p>
        <ul>
          <li>Sigmoid</li>
          <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/faaa0c014ae28ac67db5c49b3f3e8b08415a3f2b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.171ex; width:15.89ex; height:5.509ex;" alt="{\displaystyle f(x)={\frac {1}{1+e^{-x}}}}">
          <img src="img/sigmoid_act.jpg">
          <p>
            Sigmoid function takes a real-valued number and “squashes” it into range between 0 and 1, i.e.,
            <strong>σ(x)∈(0,1)</strong>In particular, large negative numbers become 0 and large positive numbers become 1. Moreover, the sigmoid function has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency
          </p>
          <li>Tanh</li>
          <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4f1b5f1173b93d23c64a0d3508028f8649a5a14e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.171ex; width:26.45ex; height:5.843ex;" alt="{\displaystyle f(x)=\tanh x={\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}}">
          <img src="img/tanh_act.jpg">
          <p>
            The hyperbolic tangent (tanh) function (used for hidden layer neuron output) is an alternative to Sigmoid function. It is defined by the formula
          </p>
          <li>Rectified Linear(ReLU)</li>
          <img id="https://t1.daumcdn.net/cfile/tistory/246B094F57F226C036" src="https://t1.daumcdn.net/cfile/tistory/246B094F57F226C036" width="244" height="91" style="width: 244px; height: 91px;">
          <img src="img/relu_act.jpg">
          <p>
            The ReLU function is more effectively than the widely used logistic sigmoid and its more practical counterpart, the hyperbolic tangent.
          </p>
          <li>Softmax</li>
          <img src="img/softmax_act.jpg">
          <p>
            In probability theory, the output of the Softmax function can be used to represent a categorical distribution, that is, a probability distribution over
            K different possible outcomes. In fact, it is the gradient-log-normalizer of the categorical probability distribution. Here is an example of Softmax application
          </p>
        </ul>
      </div>
      <div class="loss_function">
        <h2><li>Loss Function</li> </h2>
        <p>
          In most learning networks, error is calculated as the difference between the actual output and the predicted output.
        </p>
        <img src="img/loss_calculate.jpg">
        <p>
          The loss functions are helpful to train a neural network. Given an input and a target, they calculate the loss, i.e difference between output and target variable.
        </p>
      </div>
      <div class="train_network">
        <h2><a href="train_network.html"><li>Training Network</li></a> </h2>
      </div>
    </ol>
  </body>
</html>
